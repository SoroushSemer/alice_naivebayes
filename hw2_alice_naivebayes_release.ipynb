{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alice', 'adventures', 'in', 'wonderland', 'by', 'lewis', 'carroll', 'the', 'millennium', 'fulcrum', 'edition', '3', 'contents', 'chapter', 'i', 'down', 'the', 'rabbit', 'chapter', 'ii', 'the', 'pool', 'of', 'tears', 'chapter']\n",
      "corpus len:  25320\n"
     ]
    }
   ],
   "source": [
    "corpus = []\n",
    "f = open('alice_in_wonderland.txt','r')\n",
    "while(1):\n",
    "    line =  f.readline()\n",
    "    if len(line) == 0: break\n",
    "    corpus.extend(line.split())\n",
    "        \n",
    "f.close()\n",
    "corpus = ' '.join(corpus)\n",
    "\n",
    "def clean_word(word):\n",
    "    word = word.lower()\n",
    "    for punctuation in ['\"',\"'\",'.',',','-','?','!',';',':','â€”','(',')','[',']']:\n",
    "        word = word.split(punctuation)[0]\n",
    "    return word\n",
    "\n",
    "\n",
    "\n",
    "corpus = [clean_word(word) for word in corpus.split()]\n",
    "corpus = [word for word in corpus if len(word) > 0]\n",
    "print(corpus[:25])\n",
    "D = len(corpus)\n",
    "print('corpus len: ',D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word list size (number of distinct words):  2637\n"
     ]
    }
   ],
   "source": [
    "tokenize = {}\n",
    "wordlist = []\n",
    "token = 0\n",
    "for word in corpus:\n",
    "    if word not in tokenize.keys():\n",
    "        tokenize[word] = token\n",
    "        wordlist.append(word)\n",
    "        token += 1\n",
    "    \n",
    "V = len(wordlist)\n",
    "print('word list size (number of distinct words): ', V)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [9. 1. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# bin how many times a word follows another word\n",
    "counts_2gram = np.zeros((V,V))\n",
    "for i in range(1,len(corpus)):\n",
    "    token_i = tokenize[corpus[i]]\n",
    "    token_im1 = tokenize[corpus[i-1]]\n",
    "    counts_2gram[token_i,token_im1] += 1\n",
    "print(counts_2gram)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('was', 0.0007109004739336493)\n",
      "('queen', 0.0027646129541864135)\n",
      "('and', 0.00015797788309636652)\n",
      "('said', 0.0001579778830963665)\n",
      "(n = 1, 0.2500098740076622)\n"
     ]
    }
   ],
   "source": [
    "#past word as feature\n",
    "\n",
    "posterior_1word = np.zeros((V, V))\n",
    "prior = np.zeros(V)\n",
    "\n",
    "posterior_1word = np.divide(counts_2gram.T, np.sum(counts_2gram, axis=1)).T\n",
    "total_word_count = len(corpus)\n",
    "prior = np.sum(counts_2gram, axis=1) / total_word_count\n",
    "\n",
    "# classifier = np.multiply(posterior_1word, prior)\n",
    "def get_likelihood_2gram(word):\n",
    "    all_y_for_word = counts_2gram[:,tokenize[word]]\n",
    "    total_word_occurences = np.sum(counts_2gram,axis=0)\n",
    "    prob_of_x_given_y =np.divide(all_y_for_word,total_word_occurences, out=np.zeros_like(all_y_for_word), where=total_word_occurences!=0)\n",
    "    \n",
    "    return(np.multiply(prob_of_x_given_y,prior))\n",
    "\n",
    "dictionary = list(tokenize.keys())\n",
    "def pred_2gram(word):\n",
    "    likelihood = get_likelihood_2gram(word)\n",
    "    i = np.argmax(likelihood)\n",
    "    return(dictionary[i], likelihood[i])\n",
    "print(pred_2gram('alice'))\n",
    "print(pred_2gram('the'))\n",
    "print(pred_2gram('cat'))\n",
    "print(pred_2gram('turtle'))\n",
    "\n",
    "# print(classifier)\n",
    "\n",
    "\n",
    "def get_class_accuracy(n):\n",
    "    correct = 0\n",
    "    for i in range(n, len(corpus)):\n",
    "        predicted_word = pred_anygram(corpus[i-n:i])[0]\n",
    "        if(predicted_word == corpus[i]):\n",
    "            correct+=1\n",
    "    return correct / (len(corpus)-n)\n",
    "\n",
    "print(\"(n = 1, \"+ str(get_class_accuracy(1))+ \")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('cards', 0.00011848341232227489)\n",
      "('you', 5.738512847517587e-06)\n",
      "('up', 2.1392838335966295e-05)\n",
      "('miles', 1.3164823591363875e-05)\n",
      "(n = 2, 0.5104668615214472)\n"
     ]
    }
   ],
   "source": [
    "#past 2 words as features\n",
    "\n",
    "\n",
    "\n",
    "counts_3gram = np.zeros((V,V))\n",
    "for i in range(2,len(corpus)):\n",
    "    token_i = tokenize[corpus[i]]\n",
    "    token_im1 = tokenize[corpus[i-2]]\n",
    "    counts_3gram[token_i,token_im1] += 1\n",
    "\n",
    "\n",
    "posterior_2words = np.zeros((V, V))\n",
    "posterior_2words = np.divide(counts_3gram.T, np.sum(counts_3gram, axis=1)).T\n",
    "\n",
    "posterior_2gram = np.vstack([posterior_1word,posterior_2words])\n",
    "\n",
    "\n",
    "\n",
    "def get_likelihood_3gram(word2ago,word1ago):\n",
    "    \n",
    "    all_y_for_word = counts_3gram[:,tokenize[word2ago]]\n",
    "    total_word_occurences = np.sum(counts_3gram, axis=0)\n",
    "    prob_of_x2_given_y =np.divide(all_y_for_word,total_word_occurences, out=np.zeros_like(all_y_for_word), where=total_word_occurences!=0)\n",
    "    \n",
    "#     print(prob_of_x2_given_y)\n",
    "    \n",
    "    return np.multiply(get_likelihood_2gram(word1ago), prob_of_x2_given_y)\n",
    "\n",
    "def pred_3gram(word2ago,word1ago):\n",
    "    likelihood = get_likelihood_3gram(word2ago,word1ago)\n",
    "#     print(likelihood)\n",
    "    i = np.argmax(likelihood)\n",
    "    return dictionary[i], likelihood[i]\n",
    "\n",
    "print(pred_3gram('pack','of'))\n",
    "print(pred_3gram('the','mad'))\n",
    "print(pred_3gram('she','jumped'))\n",
    "print(pred_3gram('four','thousand'))\n",
    "\n",
    "\n",
    "print(\"(n = 2, \"+ str(get_class_accuracy(2))+ \")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('well', 5.263736213123415e-11)\n",
      "('girl', 1.8513033175355449e-06)\n",
      "('miles', 1.3164823591363875e-05)\n",
      "(n = 3, 0.7551052652367974)\n",
      "(n = 5, 0.9424056883270788)\n",
      "(n = 10, 0.9962465428684314)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def counts_ngram(n):\n",
    "    counts= np.zeros((V,V))\n",
    "    for i in range(n,len(corpus)): \n",
    "        token_i = tokenize[corpus[i]]\n",
    "        token_im1 = tokenize[corpus[i-n]]\n",
    "        counts[token_i,token_im1] += 1\n",
    "    return counts\n",
    "\n",
    "def get_likelihood_anygram(words):\n",
    "    total_word_count = len(corpus)\n",
    "    prior = get_likelihood_2gram(words[-1])\n",
    "    \n",
    "    for i in range(1,len(words)):\n",
    "        counts_igram = counts_ngram(i+1)\n",
    "        \n",
    "        all_y_for_word = counts_igram[:, tokenize[words[len(words)-i-1]]]\n",
    "        total_word_occurences = np.sum(counts_igram, axis=0)\n",
    "        prob_of_y_given_x = np.divide(all_y_for_word,total_word_occurences, out=np.zeros_like(all_y_for_word), where=total_word_occurences!=0)\n",
    "        \n",
    "        prior = np.multiply(prob_of_y_given_x, prior)\n",
    "    return prior\n",
    "\n",
    "def pred_anygram(words):\n",
    "    \n",
    "    likelihood = get_likelihood_anygram(words)\n",
    "    \n",
    "    i = np.argmax(likelihood)\n",
    "    return dictionary[i], likelihood[i]\n",
    "\n",
    "\n",
    "print(pred_anygram([ 'falling', 'down', 'a', 'very', 'deep']))\n",
    "print(pred_anygram(['what', 'an', 'ignorant', 'little']))\n",
    "print(pred_anygram(['four', 'thousand',]))\n",
    "\n",
    "\n",
    "print(\"(n = 3, \"+ str(get_class_accuracy(3))+ \")\")\n",
    "print(\"(n = 5, \"+ str(get_class_accuracy(5))+ \")\")\n",
    "print(\"(n = 10, \"+ str(get_class_accuracy(10))+ \")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
